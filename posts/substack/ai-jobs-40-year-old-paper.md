AI, Jobs, and the 40-Year-Old Paper We Forgot to Read
Why the real risk of GenAI isn’t job loss, but the erosion of understanding in systems we still rely on
Fayssal El Mofatiche
Feb 23, 2026

Every few decades, a new technology arrives—and we ask the same question:

Which jobs will this replace?

Thanks for reading! Subscribe for free to receive new posts and support my work.

Today, that question is aimed squarely at generative AI.
Software engineers. Analysts. Quants. Even investors.

Code is generated in seconds. Research appears instantly. Models explain markets with unsettling fluency. The narrative feels familiar:

This time is different.

But there is a quieter, older perspective that reframes the discussion—not around disappearing jobs, but around something more structural:

What happens to expertise when we stop practicing it?

A personal encounter with an old idea
I first encountered this question in 2020.

At the time, I was leading the reengineering of more than 70 front-office tools while building a modern investment platform. These systems had evolved organically over years—deeply intertwined with the workflows, intuition, and preferences of individual quants and portfolio managers.

And there was resistance.

Not just to change, but to what the change implied.

The attachment wasn’t to the interface.
It was to what the tools represented:

their models
their judgment
their edge

Beneath the surface, the concern was not:

Will this new system work?

It was:

Will I lose control?

Will my expertise still matter?

Am I being abstracted away from my own process?

That question led me to revisit what we already know about automating expert systems.

And that’s when I found Lisbeth Bainbridge’s 1983 paper:

“Ironies of Automation.”

It wasn’t written about AI.
But it reads like a diagnosis of it.

The irony that keeps repeating
Bainbridge’s central insight is deceptively simple:

Automation does not eliminate the need for expertise.
It eliminates the opportunity to develop it.

As systems become more automated, humans are pushed into supervisory roles:

monitoring dashboards

approving outputs

intervening only when something goes wrong

They are no longer engaged in normal operation—the very environment where intuition, judgment, and deep understanding are formed.

Yet when the system fails, the human remains responsible.

That is the irony.

The more advanced the system, the more it depends on human expertise—
precisely at the moment that expertise has been allowed to decay.

Fast-forward to GenAI
The current debate around GenAI is framed in terms of replacement:

“AI will replace developers”

“AI will replace analysts”

“AI will automate knowledge work”

But in practice, something more subtle is happening.

Experts are not disappearing.
They are being distanced from the work itself.

Understanding is gradually substituted with:

prompts instead of reasoning

generated outputs instead of constructed solutions

surface coherence instead of causal models

The system appears more capable.
The human becomes less engaged.

Accountability, however, does not move.

Software engineering: from construction to supervision
In software engineering, GenAI dramatically increases speed.

But it also shifts the nature of the work:

from designing systems → to reviewing generated code

from reasoning about architecture → to validating outputs

from understanding failure modes → to reacting to them

When everything works, productivity looks extraordinary.

When something breaks, the situation changes.

Debugging becomes less about why something failed and more about reconstructing what happened in a system you didn’t fully build.

Engineers are expected to resolve edge cases in systems whose internal logic they may only partially understand.

That is not just a tooling shift.

It is a shift in how expertise is formed.

Finance and investing: fluency without conviction
Finance has encountered earlier versions of this dynamic.

Risk models, pricing engines, and automated strategies did not remove experts—they relocated expertise into systems that perform well under normal conditions but become opaque under stress.

GenAI extends this pattern.

Investment narratives can be generated instantly.
Market analyses can be synthesized at scale.
Signals can be produced faster than they can be interrogated.

The outputs are fluent.
They are coherent.
They often align with prior beliefs.

And that is precisely the problem.

Over time, something subtle erodes:

conviction grounded in understanding.

When decisions are increasingly supported by systems that are not deeply examined, the link between reasoning and action weakens.

And when regimes shift—when correlations break, assumptions fail, and models drift—the critical question is no longer:

Was the system useful?

It becomes:

Who actually understands what just happened?

The real risk isn’t job loss
The dominant AI narrative focuses on displacement.

But Bainbridge points to a different failure mode:

Not fewer jobs—
but thinner expertise.

weaker intuition

reduced system-level understanding

declining ability to reason under failure

Combined with:

unchanged accountability

This is how systems become fragile.

Not because humans are removed,
but because expertise becomes:

implicit

fragmented

and eventually inaccessible

A different question to ask
So perhaps the right question is not:

Will AI replace experts?

But:

What happens when expertise is still required—
but no longer visible, practiced, or understood?

History suggests a consistent answer:

Systems that hide complexity while leaving humans responsible
do not eliminate risk.

They defer it.

And often amplify it.

Old insight, new systems
Bainbridge wrote Ironies of Automation more than 40 years ago.

The systems have changed.
The interfaces are cleaner.
The capabilities are exponentially greater.

But the underlying dynamic remains intact:

When we design systems that remove humans from understanding while keeping them accountable,
we create the conditions for failure—just at a later point in time.

GenAI does not break this pattern.

It scales it.

Closing thought
The future of AI will not be defined solely by what machines can do.

It will be defined by how we design systems around them:

whether they preserve human understanding

or quietly replace it with interface-level interaction

Because when something breaks—and something always does—
we will still turn to humans for answers.

The only question is:

Will we still have them?